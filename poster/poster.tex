%Poster do trabalho de conclusao de curso

\documentclass[final]{beamer}
\mode<presentation>{\usetheme{azul}}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{subfigure}

\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage{ragged2e}

\usepackage{mathtools}
\usepackage{amsmath,amsthm, amssymb, latexsym}
\usepackage[orientation=portrait,size=a2,scale=1.4]{beamerposter}
\usepackage[ruled]{algorithm2e}

\usepackage{enumerate}

\usepackage{snapshot} % will write a .dep file with all dependencies, allows for easy bundling

\DeclareMathSizes{17.42}{15}{14}{10}  % Math text size

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  MACROS %%%%%%%%%%%%%%%%%%%%%
\usepackage{xspace}
\newcommand{\pixel}{\emph{pixel}\xspace}
\newcommand{\pixels}{\emph{pixels}\xspace}
\newcommand{\voxel}{\emph{voxel}\xspace}
\newcommand{\voxels}{\emph{voxels}\xspace}

\newenvironment<>{spn-def}[1][\unskip]
  {\alert{\upshape\textbf{Definição #1}}\hspace*{\fill} \\
    \itshape}
  {}

\newenvironment<>{spn-thm}[1][]
  {\alert{\upshape\textbf{Teorema}} #1\hspace*{\fill} \\
    \itshape}
  {}

\listfiles
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\huge Aprendizado Automático de Sum-Product Networks}

\author{Renato Lui Geh, Orientador: Denis Deratani Mauá}
\institute[Universidade de São Paulo] % (optional, but mostly needed)
{
  Instituto de Matemática e Estatística, Universidade de São Paulo - MAC0215 Atividade Curricular
  em Pesquisa
}

\date[Novembro 2015]{Novembro, 2015}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newlength{\columnheight}
\setlength{\columnheight}{65cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{frame}
  \begin{columns}
    % ---------------------------------------------------------%
    % Set up a column
    \begin{column}{.5\textwidth}
      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
        \begin{minipage}[T]{1\textwidth} % tweaks the width, makes a new \textwidth
          \parbox[t][\columnheight]{\textwidth}{ % must be some better way to set the the height, width and textwidth simultaneously
            % Since all columns are the same length, it is all nice and tidy.  You have to get the height empirically
            % ---------------------------------------------------------%
            % fill each column with content
\vspace*{0.2cm}
            \begin{block}{Motivação}
              \justifying
              Uma distribuição de probabilidades pode ser representada por uma função multilinear
              nas variáveis da distribuição como

              \begin{equation*}\tiny
                P(x_1,...,x_n) = \sum_\alpha c_\alpha \prod_{i \in \alpha} x_i\text{.}
              \end{equation*}

              O objetivo em Modelos Gráficos Probabilísticos (PGM) é computar inferência, ou seja,
              deseja-se encontrar a probabilidade

              \begin{equation*}\tiny
                P(x_q|x_{e_1},...,x_{e_m}) = \frac{P(x_qx_{e_1}...x_{e_m})}{P(x_{e_1}...x_{e_m})}
                \text{\small onde $x_q$ é a query e $x_{e_1},...,x_{e_m}$ é a evidência.}
              \end{equation*}

              No entanto, inferência na maioria dos PGMs é intratável e, apesar de existirem
              modelos onde a inferência é, de fato, tratável, elas possuem limitações quanto a
              compactibilidade de suas representações. \\~\\

              Em 2011\cite{poon-domingos}, Pedro Domingos e Hoifung Poon introduziram um novo tipo
              de modelo probabilístico que representa eficientemente uma função multilinear através
              de um digrafo acíclico enraizado (DAG), cuja inferência é sempre tratável e ainda
              assim é mais flexível que muitos outros modelos. Por meio de experimentos também
              comprovou-se que tanto inferência quanto aprendizado foram mais rápidos e precisos
              que outras redes profundas. \\~\\

              O objetivo desse estudo é aprender a definição, estrutura e propriedades de
              Sum-Product Networks e em seguida estudar os vários tipos de aprendizado que podemos
              efetuar neste modelo.

            \vspace*{0.2cm}
            \end{block}
            \vspace*{0.2cm}

            \begin{block}{Sum-Product Networks}
              \justifying
              Uma Sum-Product Network (SPN) tem definição recursiva. Seja $S$ uma SPN:
              \begin{itemize}
                \item Uma distribuição monovariável $P(X_i)$ é uma SPN.
                \item $w_iS_i(X_\alpha)+w_jS_j(X_\beta)$ é uma SPN se $\alpha=\beta$ e pesos $w_i,
                  w_j\geq0$.\hfill\small{(Definição 2)}
                \item \normalsize{$S_i(X_\alpha) \cdot S_j(X_\beta)$ é uma SPN se}
                  $\alpha\cap\beta=\emptyset$.\hfill\small{(Definição 3)}
              \end{itemize}

              \begin{figure}[h]
                \includegraphics[scale=0.3]{spn_1.png}
                \includegraphics[scale=0.3]{spn_2.png}
                \caption{A esquerda uma SPN implementando uma naive Bayes mixture model. A direita
                  uma SPN implementando uma junction tree. Fonte: Poon e
                  Domingos\cite{poon-domingos}.}
              \end{figure}

              \begin{spn-def}[1] Uma SPN $S$ é válida sse $\exists P:S(x)=P(x)$, $\forall x$.
              \end{spn-def} \\~\\

              \begin{spn-def}[2] Uma SPN é \emph{completa} sse todos os filhos do mesmo nó soma tem
                mesmo escopo.
              \end{spn-def} \\~\\

              \begin{spn-def}[3] Uma SPN é \emph{consistente} sse nenhuma variável aparece negada
                em um filho de um nó produto e não-negada em outro.
              \end{spn-def} \\~\\

              \begin{spn-thm} Uma SPN é válida se ela é completa e consistente.
              \end{spn-thm} \\~\\

              SPNs válidas são desejáveis pois uma SPN válida computa $P(x_1,...,x_n)$ em tempo
              linear em seu tamanho, além de completude e consistência permitirem que a inferência
              da SPN seja garantidamente eficiente.
\vspace*{0.2cm}
            \end{block}
\vspace*{0.2cm}
            \begin{block}{Aprendizado}
                \justifying
                Pode-se dividir aprendizado de SPNs em duas classes:
                \begin{description}
                  \item[1] A partir de um DAG da SPN pré-definido, aprendemos os pesos do digrafo.
                  \item[2] Ambos DAG e pesos são desconhecidos e são aprendidos.\\~\\
                \end{description}

                O algoritmo proposto em \cite{poon-domingos} segue a primeira classe e é mostrado
                na seção seguinte. A partir de uma SPN densa e válida podemos aprender os pesos por
                Gradient Descent ou Expectation-Maximization (EM). \\~\\

                Gens e Domingos\cite{gens-domingos} propõem um outro método de aprendizado que
                explora a expressividade da SPN aprendendo-se não só os pesos como o próprio DAG.
                Para aprender o digrafo pode-se maximizar o estimador de semelhança $\max_S P(X^1,
                ...,X^N|S)$, onde $X^1,...,X^N$ são os conjuntos de dados.
           \end{block}
          }
        \end{minipage}
      \end{beamercolorbox}
    \end{column}
    % ---------------------------------------------------------%
    % end the column

    % ---------------------------------------------------------%
    % Set up a column
    \begin{column}{.5\textwidth}
      \begin{beamercolorbox}[center,wd=\textwidth]{postercolumn}
        \begin{minipage}[T]{1\textwidth} % tweaks the width, makes a new \textwidth
          \parbox[t][\columnheight]{\textwidth}{ % must be some better way to set the the height, width and textwidth simultaneously
            % Since all columns are the same length, it is all nice and tidy.  You have to get the height empirically
            % ---------------------------------------------------------%
            % fill each column with content

\vspace*{0.2cm}
            \begin{block}{Algoritmo de Aprendizado de Pesos}

              \begin{algorithm}[H]
                \NoCaptionOfAlgo
                \KwIn{Conjunto $D$ de instâncias sobre variáveis $X$.}
                \KwOut{Uma SPN com estrutura e parâmetros construídos por aprendizado.}
                \tcc{Cria uma SPN inicial que seja válida.}
                $S \leftarrow$ GenerateDenseSPN($X$)\;
                InitializeWeights($S$)\;
                \Repeat{convergência}{
                  \ForAll{$d \in D$}{
                    \tcc{Atualiza pesos por Gradient Descent ou EM.}
                    UpdateWeights($S$, Inference($S$, $d$))\;
                  }
                }
                \tcc{Apara arestas com peso $w_{ij}=0$ e nós não-raíz sem pais.}
                $S \leftarrow$ PruneZeroWeights($S$)\;
                \KwRet{$S$}
              \end{algorithm}
\vspace*{0.2cm}
            \end{block}
\vspace*{0.2cm}
            \begin{block}{Experimentos}
            \justifying
              Os experimentos mostrados a seguir foram extraídos a partir da implementação do
              algoritmo mostrado na seção anterior e mostram os resultados do código
              \cite{website:poon-domingos-code} implementado por Domingos e Poon e citados em
              \cite{poon-domingos}.

              \begin{figure}[h]
                \includegraphics[scale=0.3425]{completions/plane0.png}
                \includegraphics[scale=0.3425]{completions/face0.png} \\
                \includegraphics[scale=0.3425]{completions/stop0.png}
                \includegraphics[scale=0.3425]{completions/dollar0.png}
                \caption{A saída do algoritmo consiste na compleção do lado esquerdo das imagens
                  a partir de um conjunto de treino. Para cada par de imagens, a imagem da esquerda
                  é a original, enquanto que a direita tem a metade esquerda completada pela SPN
                  e a outra metade igual a da original como evidência.}
              \end{figure}

              \begin{table}[h]
                \begin{center}
                  \begin{tabular}{l | r | r | r}
                    Arquitetura & Rostos & Motos & Carros \\
                    \hline
                    SPN & 99\% & 99\% & 98\% \\
                    CDBN & 95\% & 81\% & 87\% \\
                  \end{tabular}
                \end{center}
                \caption{\justifying Taxa média de acertos em uma comparação entre SPNs e CDBNs
                  (Convolutional Deep Belief Networks) em classificação (reconhecimento) de 
                  imagens. SPNs obtiveram resultados quase perfeitos em reconhecimento.}
              \end{table}

              Pode-se ver que os resultados das SPNs são muito promissores e, dado que o algoritmo
              produzido por Domingos e Poon não toma muita vantagem da expressividade da estrutura
              local de SPNs, é fácil notar que ainda há muito espaço para melhorias.

\vspace*{0.2cm}
            \end{block}
\vspace*{0.2cm}
            \begin{block}{Trabalhos futuros}
              \justifying
                Pretende-se estudar a implementação do método de aprendizado proposto por Poon e
                Domingos\cite{poon-domingos}, realizar outros experimentos com este algoritmo e
                explorar mais a fundo as propriedades de uma SPN.\\~\\

                Em seguida planeja-se estudar outros tipos de aprendizado em SPNs, principalmente
                métodos que estejam contidos na classe 2 de aprendizado e portanto tomem vantagem
                da estrutura local de SPNs, como o introduzido por Gens e
                Domingos\cite{gens-domingos}, buscas gulosas e clustering por Dennis e
                Ventura\cite{clustering, greedy-search} e Non-Parametric Bayesian Sum-Product
                Networks\cite{non-parametric-bayesian}.

            \end{block}
\vspace*{0.2cm}
            \begin{block}{Referências}
            \scriptsize
            \bibliographystyle{alpha}
            \bibliography{references}
\vspace*{0.2cm}
            \end{block}
            \vfill
          }
        \end{minipage}
      \end{beamercolorbox}
    \end{column}
    % ---------------------------------------------------------%
    % end the column


  \end{columns}
\end{frame}


\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-PDF-mode: t
%%% End:
